{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-cherry",
   "metadata": {},
   "source": [
    "### Calculation of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6587,  1.2707, -0.9851], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(size=(3,), requires_grad=True)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-standing",
   "metadata": {},
   "source": [
    "Whenever a PyTorch tensor with ```requires_grad=True``` is used in a computation, a computational graph is created that is utilized during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indonesian-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3413, 3.2707, 1.0149], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-atmosphere",
   "metadata": {},
   "source": [
    "The attribute ```grad_fn``` is utilized for the backpropagation of the gradient. As a value is being added, the value of the attribute is ```AddBackward0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prospective-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.0178, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "z  = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-zoning",
   "metadata": {},
   "source": [
    "It can be seen that the value of ```grad_fn``` is now ```MulBackward0``` as the operation is a multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-nelson",
   "metadata": {},
   "source": [
    "Computation of the gradients involves invoking the ```backward()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "original-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-phenomenon",
   "metadata": {},
   "source": [
    "```backward()``` only works on scalar values. For the method to work with non-scalar tensors, one must keep in mind that the gradients are calculated by a **Jacobian product** and would thus need a vector to be provided that would match the shape of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-darkness",
   "metadata": {},
   "source": [
    "The gradients of a tensor can be found using the ```grad``` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dated-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7883, 4.3610, 1.3532])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-eclipse",
   "metadata": {},
   "source": [
    "**Prevention of gradient tracking**\n",
    "- ```x.requires_grad_(False)```\n",
    "- ```x.detach()```\n",
    "- ```with torch.no_grad():```\n",
    "  ```x ...```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-founder",
   "metadata": {},
   "source": [
    "To set the ```grad``` attribute to reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-eating",
   "metadata": {},
   "source": [
    "In a training loop, the gradient in the ```grad``` attribute accumulates with each iteration. This is prevented by resetting the values to zero with the ```.grad.zero_()``` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-denial",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-minnesota",
   "metadata": {},
   "source": [
    "For each operation, PyTorch creates a computational graph that allows for calculation of local gradients which makes gradient calculation simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-mississippi",
   "metadata": {},
   "source": [
    "Training overview:\n",
    "- Forward pass: Computation of loss\n",
    "- Computation of local gradients\n",
    "- Backward pass: Computation of $\\frac{\\partial Loss}{\\partial Weights}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-stewart",
   "metadata": {},
   "source": [
    "### Manual training loop using ```backward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0\tweight:\t0.29999998211860657\tloss:\t30.0\n",
      "epoch:\t10\tweight:\t1.6653136014938354\tloss:\t1.1627856492996216\n",
      "epoch:\t20\tweight:\t1.934108853340149\tloss:\t0.0450688973069191\n",
      "epoch:\t30\tweight:\t1.987027645111084\tloss:\t0.0017468547448515892\n",
      "epoch:\t40\tweight:\t1.9974461793899536\tloss:\t6.770494655938819e-05\n",
      "9.997042655944824\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return((y - y_hat)**2).mean()\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    l = loss(y, y_pred)\n",
    "    l.backward() # gradient calculation\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if (epoch % 10 == 0):\n",
    "        print(f\"epoch:\\t{epoch}\\tweight:\\t{w.item()}\\tloss:\\t{l}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    print(forward(5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-heading",
   "metadata": {},
   "source": [
    "### Automating the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-lobby",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1) Design model (input, output size, forward size)\n",
    "\n",
    "2) Construct loss and optimizer\n",
    "\n",
    "3) Training loop:\n",
    "\n",
    "    - forward pass: compute prediction\n",
    "    \n",
    "    - backward pass: gradients\n",
    "    \n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "known-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-benjamin",
   "metadata": {},
   "source": [
    "```torch.nn.Linear```: Applies a linear transformation to the incoming data: \n",
    "$y = xA^T + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-necklace",
   "metadata": {},
   "source": [
    "When using Pytorch utilities to train a model:\n",
    "- Define the model: Can be done using layers given in ```torch.nn```.\n",
    "- Define the loss and optimizer: Optimizers are under ```torch.optim```.\n",
    "- One forward pass is done by simpling running the model on the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-checkout",
   "metadata": {},
   "source": [
    "Data to be passed to a model cannot be 1-dimensional. Shape should be ```(n_samples, n_features)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flush-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0\tweight:\t1.1769566535949707\tloss:\t7.69447135925293\n",
      "epoch:\t10\tweight:\t1.813909649848938\tloss:\t0.20492786169052124\n",
      "epoch:\t20\tweight:\t1.9179493188858032\tloss:\t0.010814713314175606\n",
      "epoch:\t30\tweight:\t1.9362235069274902\tloss:\t0.005471664480865002\n",
      "epoch:\t40\tweight:\t1.9406569004058838\tloss:\t0.005031246691942215\n",
      "9.713160514831543\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# Defining the model\n",
    "# model = nn.Linear(in_features=n_features, \n",
    "#                 out_features=n_features,\n",
    "#                 bias=False)\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    l = loss(y, y_pred)\n",
    "    l.backward() # gradient calculation\n",
    "\n",
    "    optimizer.step() # Automatic updation of weights\n",
    "    optimizer.zero_grad() # Automatic resetting of weights\n",
    "    \n",
    "    if (epoch % 10 == 0):\n",
    "        [w,b] = model.parameters()\n",
    "        print(f\"epoch:\\t{epoch}\\tweight:\\t{w[0][0].item()}\\tloss:\\t{l}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    print(forward(5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-bubble",
   "metadata": {},
   "source": [
    "### Custom model\n",
    "Follows the same setup as TensorFlow. The method for one forward pass is ```forward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "weird-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                               out_features=output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegression(n_features, n_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
