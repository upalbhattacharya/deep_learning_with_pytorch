{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-microphone",
   "metadata": {},
   "source": [
    "### Softmax and Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protective-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([2.0, 1.0, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-librarian",
   "metadata": {},
   "source": [
    "PyTorch has a softmax method called ```softmax()``` that takes one argument ```dim``` that specifies the axis/dimension along which softmax is to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "former-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-premises",
   "metadata": {},
   "source": [
    "Using Cross Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-command",
   "metadata": {},
   "source": [
    "PyTorch's ```CrossEntropyLoss()``` applies the ```LogSoftmax``` and ```NLLLoss()``` and so **the predictions should be passed without applying a softmax**. Additionally, the **targets should be given as class labels and not as one-hot vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifth-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "1.243420124053955\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([0])\n",
    "#n_samples x n_classes\n",
    "y_pred_good = torch.Tensor([[2.0, 1.0, 0.1]])\n",
    "y_pred_bad = torch.Tensor([[0.5, 1.0, 0.3]])\n",
    "l1 = loss(y_pred_good, y)\n",
    "l2 = loss(y_pred_bad, y)\n",
    "print(l1.item())\n",
    "print(l2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-factor",
   "metadata": {},
   "source": [
    "Getting the actual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-credits",
   "metadata": {},
   "source": [
    "To get the prediction as the maximum value after the application of softmax (or without it), the ```max()``` function can be used. It takes the argument ```dim``` which specifies the axis/dimension along which the maximum is to be found. It returns a tuple consisting of the maximum values contained in ```tensor_name.values``` and their corresponding indices in ```tensor_name.indices```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "appropriate-cancellation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "pred_good = torch.max(torch.softmax(y_pred_good, 1), 1)\n",
    "pred_bad = torch.max(torch.softmax(y_pred_bad, 1), 1)\n",
    "print(pred_good.indices)\n",
    "print(pred_bad.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-future",
   "metadata": {},
   "source": [
    "In a binary classification problem, when using ```BCELoss```, sigmoid is to be applied at the end. In a multi-class classification problem, when using ```CrossEntropyLoss```, softmax is not to be applied at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-miracle",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-reputation",
   "metadata": {},
   "source": [
    "PyTorch provides all the standard activation functions. These are provided either as modules from ```torch.nn``` or as methods directly from torch:\n",
    "- ```torch.nn.Sigmoid``` or ```torch.sigmoid```\n",
    "- ```torch.nn.Softmax``` or ```torch.softmax```\n",
    "- ```torch.nn.ReLU``` or ```torch.relu```\n",
    "- ```torch.nn.Tanh``` or ```torch.tanh```\n",
    "\n",
    "However certain activation functions are only available under ```torch.nn.functional``` such as:\n",
    "- ```torch.nn.LeakyReLU``` or ```torch.nn.functional.leaky_relu```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-version",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
