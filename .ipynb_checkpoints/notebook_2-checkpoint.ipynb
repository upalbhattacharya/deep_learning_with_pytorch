{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-cherry",
   "metadata": {},
   "source": [
    "### Calculation of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "balanced-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6297,  1.8933, -0.7387], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(size=(3,), requires_grad=True)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-standing",
   "metadata": {},
   "source": [
    "Whenever a PyTorch tensor with ```requires_grad=True``` is used in a computation, a computational graph is created that is utilized during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indonesian-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3703, 3.8933, 1.2613], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-atmosphere",
   "metadata": {},
   "source": [
    "The attribute ```grad_fn``` is utilized for the backpropagation of the gradient. As a value is being added, the value of the attribute is ```AddBackward0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "prospective-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.4175, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 2\n",
    "z  = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-zoning",
   "metadata": {},
   "source": [
    "It can be seen that the value of ```grad_fn``` is now ```MulBackward0``` as the operation is a multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-nelson",
   "metadata": {},
   "source": [
    "Computation of the gradients involves invoking the ```backward()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "original-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() # dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-phenomenon",
   "metadata": {},
   "source": [
    "```backward()``` only works on scalar values. For the method to work with non-scalar tensors, one must keep in mind that the gradients are calculated by a **Jacobian product** and would thus need a vector to be provided that would match the shape of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-darkness",
   "metadata": {},
   "source": [
    "The gradients of a tensor can be found using the ```grad``` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dated-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3081, 20.7642,  6.7272])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-eclipse",
   "metadata": {},
   "source": [
    "**Prevention of gradient tracking**\n",
    "- ```x.requires_grad_(False)```\n",
    "- ```x.detach()```\n",
    "- ```with torch.no_grad():```\n",
    "  ```x ...```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-founder",
   "metadata": {},
   "source": [
    "To set the ```grad``` attribute to reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acknowledged-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-eating",
   "metadata": {},
   "source": [
    "In a training loop, the gradient in the ```grad``` attribute accumulates with each iteration. This is prevented by resetting the values to zero with the ```.grad.zero_()``` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-denial",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-minnesota",
   "metadata": {},
   "source": [
    "For each operation, PyTorch creates a computational graph that allows for calculation of local gradients which makes gradient calculation simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-mississippi",
   "metadata": {},
   "source": [
    "Training overview:\n",
    "- Forward pass: Computation of loss\n",
    "- Computation of local gradients\n",
    "- Backward pass: Computation of $\\frac{\\partial Loss}{\\partial Weights}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-stewart",
   "metadata": {},
   "source": [
    "**Manual** training loop using ```backward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "painted-convergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0\tweight:\t0.29999998211860657\tloss:\t30.0\n",
      "epoch:\t10\tweight:\t1.6653136014938354\tloss:\t1.1627856492996216\n",
      "epoch:\t20\tweight:\t1.934108853340149\tloss:\t0.0450688973069191\n",
      "epoch:\t30\tweight:\t1.987027645111084\tloss:\t0.0017468547448515892\n",
      "epoch:\t40\tweight:\t1.9974461793899536\tloss:\t6.770494655938819e-05\n",
      "9.997042655944824\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return((y - y_hat)**2).mean()\n",
    "\n",
    "lr = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    l = loss(y, y_pred)\n",
    "    l.backward() # gradient calculation\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if (epoch % 10 == 0):\n",
    "        print(f\"epoch:\\t{epoch}\\tweight:\\t{w.item()}\\tloss:\\t{l}\")\n",
    "        \n",
    "with torch.no_grad():\n",
    "    print(forward(5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-guitar",
   "metadata": {},
   "source": [
    "Timestamp 1:15:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-fleece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
